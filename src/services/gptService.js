import axios from "axios";
import Conversation from "../models/Conversation.js";
import dotenv from "dotenv";
dotenv.config();

let dbInstance;

export function setDB(db) {
  dbInstance = db;
}

export const authorizedNumbers = ["554195194485"];

const cache = new Map();
export function getCached(prompt) { return cache.get(prompt); }
export function setCached(prompt, resposta) { cache.set(prompt, resposta); }

const economicalModel = "gpt-4o-mini";
const MAX_TOKENS = 300;
const TEMPERATURE = 0.7;

export async function getGPTResponse(userMessage, imageUrl = null, userId, phoneNumber) {
  if (phoneNumber && !authorizedNumbers.includes(phoneNumber)) {
    console.log(`‚ùå Usu√°rio n√£o autorizado: ${phoneNumber}`);
    return "Desculpe, voc√™ n√£o est√° autorizado a usar este servi√ßo.";
  }

  try {
    const history = await Conversation.find({ from: userId }).sort({ createdAt: 1 });

    const messages = [
      {
        role: "system",
        content: `
Voc√™ √© Donna, assistente executiva perspicaz, elegante e humanizada.
- Ajuda em administra√ß√£o, legisla√ß√£o, RH e neg√≥cios.
- Poliglota: responda no idioma da mensagem do usu√°rio.
- D√° dicas estrat√©gicas e conselhos.
- Ajuda com lembretes e compromissos.
- Responde de forma natural, personalizada e com humor ou empatia.
        `,
      },
    ];

    history.forEach(h => {
      messages.push({ role: h.role === "assistant" ? "assistant" : "user", content: h.content });
    });

    let userContent = userMessage || "";
    if (imageUrl) userContent += `\nüì∑ Imagem recebida: ${imageUrl}`;
    messages.push({ role: "user", content: userContent });

    const cached = getCached(userContent);
    if (cached) return cached;

    const datasetAnswer = null; // aqui voc√™ pode chamar datasetService.buscarRespostaDataset(userContent)
    if (datasetAnswer) {
      setCached(userContent, datasetAnswer);
      return datasetAnswer;
    }

    try {
      const response = await axios.post(
        "https://api.openai.com/v1/chat/completions",
        { model: economicalModel, messages, max_tokens: MAX_TOKENS, temperature: TEMPERATURE },
        { headers: { Authorization: `Bearer ${process.env.OPENAI_API_KEY}`, "Content-Type": "application/json" }, timeout: 10000 }
      );

      const answer = response.data.choices?.[0]?.message?.content?.trim() || "Desculpe, n√£o consegui gerar uma resposta.";
      const limitedAnswer = answer.length > 150 ? answer.slice(0, 150) : answer;
      setCached(userContent, limitedAnswer);
      return limitedAnswer;

    } catch (modelError) {
      console.error("‚ö†Ô∏è Erro GPT econ√¥mico:", modelError.message || modelError);
      const fallbackAnswer = "Desculpe, n√£o consegui responder agora. Tente novamente mais tarde.";
      setCached(userContent, fallbackAnswer);
      return fallbackAnswer;
    }

  } catch (error) {
    console.error("‚ùå Erro geral no GPT:", error.message || error);
    return "Desculpe, tive um problema para responder agora.";
  }
}

